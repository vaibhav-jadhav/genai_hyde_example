{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395ad14d-fd98-45bd-ad2e-59cbeb533c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain langchain-community langchain-openai langchain-qdrant qdrant-client python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d70d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"python.pdf\"\n",
    "COLLECTION_NAME = \"learning_langchain-hyde\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "THRESHOLD = 0.50\n",
    "QUERY = \"How to loop over two lists together?\"\n",
    "\n",
    "# Initialize clients\n",
    "key=\"\" # insert your Open Ai key here\n",
    "# initilize embedding using openai model\n",
    "embedder = OpenAIEmbeddings(api_key=key,model=\"text-embedding-3-small\")\n",
    "qdrant_client = QdrantClient( url=QDRANT_URL)\n",
    "openai_client = OpenAI(api_key=key)\n",
    "system_prompt = \"\"\"\n",
    "You are an AI Assistant who can take users' Python documentation queries and answer how to perform operations in Python.\n",
    "The user might be a newbie and may ask vague or worded-differently queries. Answer them properly.\n",
    "Your answers must be short and correct.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e7466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads a PDF file from the given path and prepares it for embedding.\n",
    "def load_and_split_pdf(pdf_path: str):\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "    return docs  # or splitter.split_documents(docs) if document is too big\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcacfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# insert splitted docs into databse(e.g.Qdrant)\n",
    "def inject_into_qdrant(split_docs):\n",
    "    print(\"‚öôÔ∏è Injecting documents into Qdrant...\")\n",
    "    vector_store = QdrantVectorStore.from_documents(\n",
    "        documents=[],\n",
    "        url=QDRANT_URL,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embedder,\n",
    "        force_recreate=True\n",
    "    )\n",
    "    vector_store.add_documents(documents=split_docs)\n",
    "    print(\"‚úÖ Injection done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ac5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Performs a semantic search on the vector database using the provided query.\n",
    "def search_with_threshold(query: str):\n",
    "    retriever = QdrantVectorStore.from_existing_collection(\n",
    "        url=QDRANT_URL,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embedder\n",
    "    )\n",
    "    results = retriever.similarity_search_with_score(query, k=3)\n",
    "    # Filters out results that have a similarity score lower than the defined threshold.\n",
    "    filtered = [(doc, score) for doc, score in results if score >= THRESHOLD]\n",
    "\n",
    "    if filtered:\n",
    "        print(\"\\n‚úÖ Relevant Chunks Found:\\n\")\n",
    "        for doc, score in filtered:\n",
    "            print(f\"Score: {score:.2f}\")\n",
    "            print(\"Chunk:\", doc.page_content)\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"\\n‚ùå No relevant data found for this query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31690fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  generated hypothetical answer for user query\n",
    "def generate_hypothetical_answer(query: str) -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": system_prompt },\n",
    "            { \"role\": \"user\", \"content\": query }\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    print(\"\\nü§ñ Hypothetical Answer (HyDE):\\n\", answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca79467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 14 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Injecting documents into Qdrant...\n",
      "‚úÖ Injection done!\n",
      "\n",
      "üîç Searching without HyDE:\n",
      "\n",
      "‚ùå No relevant data found for this query.\n",
      "\n",
      "ü§ñ Hypothetical Answer (HyDE):\n",
      " You can loop over two lists together using the `zip()` function in Python. Here's an example:\n",
      "\n",
      "```python\n",
      "list1 = ['a', 'b', 'c']\n",
      "list2 = [1, 2, 3]\n",
      "for item1, item2 in zip(list1, list2):\n",
      "    print(item1, item2)\n",
      "```\n",
      "\n",
      "This will print:\n",
      "```\n",
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "```\n",
      "Please note that if the lists are not of the same length, `zip()` stops creating tuples when the first list ends.\n",
      "\n",
      "üîç Searching with HyDE-enhanced query:\n",
      "\n",
      "‚úÖ Relevant Chunks Found:\n",
      "\n",
      "Score: 0.52\n",
      "Chunk: \"The\n",
      " \n",
      "zip()\n",
      " \n",
      "function\n",
      " \n",
      "in\n",
      " \n",
      "Python\n",
      " \n",
      "allows\n",
      " \n",
      "you\n",
      " \n",
      "to\n",
      " \n",
      "iterate\n",
      " \n",
      "over\n",
      " \n",
      "multiple\n",
      " \n",
      "iterables\n",
      " \n",
      "(like\n",
      " \n",
      "lists)\n",
      " \n",
      "in\n",
      " \n",
      "parallel.\"\n",
      " \n",
      " \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load & Inject PDF\n",
    "docs = load_and_split_pdf(PDF_PATH)\n",
    "inject_into_qdrant(docs)\n",
    "\n",
    "# Step 2: Search without HyDE\n",
    "print(\"\\nüîç Searching without HyDE:\")\n",
    "search_with_threshold(QUERY)\n",
    "\n",
    "# Step 3: Generate Hypothetical Answer via LLM\n",
    "hypo_query = generate_hypothetical_answer(QUERY)\n",
    "\n",
    "# Step 4: Search again with HyDE-enhanced query\n",
    "print(\"\\nüîç Searching with HyDE-enhanced query:\")\n",
    "search_with_threshold(hypo_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6a6ed-2472-4ed4-b1c2-656c0465e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
